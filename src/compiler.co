import "src/tokenizer.co" tokenizer;
import "src/writer.co" writer;
import "src/printer.co" printer;
import "src/source.co" source;
import "src/syscalls.co" syscalls;
import "src/identifiers.co" idents;
import "src/identifier_types.co" itypes;

zeroes import_file_root[8];
zeroes parse_top_level_ptr[8];

comptime primary_expr_only = 0;
comptime any_unambigous_expr = 1;

fn parse_comptime_value(mode) [lhs, rhs, extra] {
    switch(tokenizer.peek_type()) {
        tokenizer.error("Expected primary expression");
        endcase;

    case tokenizer.bitnot:
        tokenizer.discard();
        lhs = ~parse_comptime_value(primary_expr_only);
        endcase;

    case tokenizer.subtraction:
        tokenizer.discard();
        lhs = -parse_comptime_value(primary_expr_only);
        endcase;

    case tokenizer.open_paren:
        tokenizer.discard();
        lhs = parse_comptime_value(any_unambigous_expr);
        tokenizer.expect_token(tokenizer.closing_paren, "Expected `)` after expression");
        tokenizer.discard();
        endcase;

    case tokenizer.int_literal:
        lhs = tokenizer.peek_value();
        tokenizer.discard();
        endcase;

    case tokenizer.identifier:
        @todo("parse_comptime_value ident");
    }

    if(primary_expr_only) {
        return lhs;
    } else {
        switch(tokenizer.peek_type()) {
            @todo("parse_comptime_value default op");
        case tokenizer.closing_paren:
        case tokenizer.closing_curly_brace:
        case tokenizer.closing_square_bracket:
        case tokenizer.end_of_file:
        case tokenizer.comma:
        case tokenizer.semicolon:
        case tokenizer.colon:
        case tokenizer.dot_dot_dot:
            return lhs;

        case tokenizer.less_than:
            tokenizer.discard();
            return lhs < parse_comptime_value(primary_expr_only);

        case tokenizer.less_than_equal:
            tokenizer.discard();
            return lhs <= parse_comptime_value(primary_expr_only);

        case tokenizer.greater_than:
            tokenizer.discard();
            return lhs > parse_comptime_value(primary_expr_only);

        case tokenizer.greater_than_equal:
            tokenizer.discard();
            return lhs >= parse_comptime_value(primary_expr_only);

        case tokenizer.equals:
            tokenizer.discard();
            return lhs == parse_comptime_value(primary_expr_only);

        case tokenizer.not_equals:
            tokenizer.discard();
            return lhs == parse_comptime_value(primary_expr_only);

        case tokenizer.bitor:
            tokenizer.discard();
            return lhs | parse_comptime_value(primary_expr_only);

        case tokenizer.bitxor:
            tokenizer.discard();
            return lhs ^ parse_comptime_value(primary_expr_only);

        case tokenizer.bitand:
            tokenizer.discard();
            return lhs & parse_comptime_value(primary_expr_only);

        case tokenizer.division:
            tokenizer.discard();
            return lhs / parse_comptime_value(primary_expr_only);

        case tokenizer.modulus:
            tokenizer.discard();
            return lhs % parse_comptime_value(primary_expr_only);

        case tokenizer.multiplication:
            tokenizer.discard();
            return lhs * parse_comptime_value(primary_expr_only);

        case tokenizer.addition:
            tokenizer.discard();
            return lhs + parse_comptime_value(primary_expr_only);

        case tokenizer.subtraction:
            tokenizer.discard();
            return lhs - parse_comptime_value(primary_expr_only);

        case tokenizer.question_mark:
            tokenizer.discard();
            // Ternary operator
            extra = lhs;

            lhs = parse_comptime_value(any_unambigous_expr);
            tokenizer.expect_token(tokenizer.colon, "Expected `:` after expression");
            tokenizer.discard();
            rhs = parse_comptime_value(any_unambigous_expr);

            if(extra) {
                return lhs;
            } else {
                return rhs;
            }
        }
    }
}

fn parse_comptime_decl() [value_node, value] {
    tokenizer.discard();

    value_node = tokenizer.expect_token(tokenizer.identifier, "Expected identifier after `comptime`");
    tokenizer.discard();
    @assert(idents.node_get_type(value_node) == itypes.none);

    tokenizer.expect_token(tokenizer.assignment, "Expected `=` after identifier");
    tokenizer.discard();

    value = parse_comptime_value(any_unambigous_expr);
    idents.node_set_value(value_node, value);
    idents.node_set_type(value_node, itypes.comptime_int);

    tokenizer.expect_token(tokenizer.semicolon, "Expected `;` after comptime decl");
    tokenizer.discard();
}

fn parse_block() {

}

fn parse_enum_decl() [next_value, node] {
    tokenizer.discard();

    tokenizer.expect_token(tokenizer.open_curly_brace, "Expected `{` after `enum`");
    tokenizer.discard();

    next_value = 0;

    loop {
        if(tokenizer.peek_type() == tokenizer.closing_curly_brace) {
            tokenizer.discard();
            tokenizer.expect_token(tokenizer.semicolon, "Expected `;` after enum decl");
            tokenizer.discard();
            return;
        } else {
            node = tokenizer.expect_token(tokenizer.identifier, "Expected identifier or `}`");
            tokenizer.discard();

            @assert(idents.node_get_type(node) == itypes.none);

            if(tokenizer.peek_type() != tokenizer.comma) {
                tokenizer.expect_token(tokenizer.assignment, "Expected `=` or `,` after enum member identifier");
                tokenizer.discard();
                next_value = parse_comptime_value(any_unambigous_expr);
            } else {}
            idents.node_set_type(itypes.comptime_int);
            idents.node_set_value(next_value);
            next_value += 1;

            tokenizer.expect_token(tokenizer.comma, "Expected `,` after enum member value");
            tokenizer.discard();
            continue;
        }
    }
}

fn set_source_file(file_path) [fd] {
    fd = syscalls.open(file_path, syscalls.O_RDONLY);
    source.switch_file(fd);
}

fn parse_additional_file(file_path, new_node) [fd, root_stash, file_context_stash[source.file_context_size]] {
    // Store our current context, the imported file has no idea!
    root_stash = tokenizer.current_file_root[0];
    source.stash_file_info(file_context_stash);

    set_source_file(file_path);

    @call(parse_top_level_ptr[0]);

    // Return to monke
    source.restore_file_info(file_context_stash);
    tokenizer.current_file_root[0] = root_stash;
}

fn parse_file_if_needed(file_path) [node] {
    node = import_file_root[0];
    node = idents.lookup(file_path, node);

    switch(idents.node_get_type(node)) {
        printer.print_string("Bad filename ident type!\n");
        printer.exit(1);

    case itypes.none:
        printer.print_string("Unknown file, let's parse it!\n");

        idents.node_set_type(node, itypes.partially_parsed_filename);

        parse_additional_file(file_path, node);

        idents.node_set_type(node, itypes.fully_parsed_filename);
        return idents.node_get_attribute(node);

    case itypes.fully_parsed_filename:
        printer.print_string("File already parsed, nothing to do\n");
        return idents.node_get_attribute(node);        

    case itypes.partially_parsed_filename:
        printer.print_string("Circular import detected!\n");
        printer.exit(1);
    }
}

fn parse_import() [file_root, ident_node] {
    tokenizer.discard();

    tokenizer.expect_token(tokenizer.string_literal, "Expected a string literal after `import`.");
    tokenizer.discard();
    file_root = parse_file_if_needed(tokenizer.buffer);

    tokenizer.expect_token(tokenizer.identifier, "Expected an identifier after filename.");
    ident_node = tokenizer.peek_value();
    tokenizer.discard();

    // Assert the identifier is unused
    @assert(idents.node_get_type(ident_node) == itypes.none);

    idents.node_set_type(ident_node, itypes.variable_scope);
    idents.node_set_attribute(ident_node, file_root);

    tokenizer.expect_token(tokenizer.semicolon, "Expected `;` after import");
}

fn add_builtins_to_current_file() {

}

fn parse_top_level() {
    printer.print_string("Top level!\n");
    tokenizer.current_file_root[0] = idents.alloc();
    add_builtins_to_current_file();

    loop {
        switch(tokenizer.peek_type()) {
            @todo("parse_top_level default");

        case tokenizer.keyword_zeroes:
            @todo("parse_top_level zeroes");

        case tokenizer.keyword_fn:
            @todo("parse_top_level fn");

        case tokenizer.keyword_enum:
            parse_enum_decl();
            continue;

        case tokenizer.keyword_comptime:
            parse_comptime_decl();
            continue;

        case tokenizer.keyword_import:
            parse_import();
            continue;
        }
    }
}

fn init(base_addr) {
    idents.init();
    writer.init(base_addr);
    import_file_root[0] = idents.alloc();
    parse_top_level_ptr[0] = parse_top_level;
}
